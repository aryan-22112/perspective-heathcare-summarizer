{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11422340,"sourceType":"datasetVersion","datasetId":7153505}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate transformers datasets accelerate bitsandbytes peft rouge_score nltk bert_score sentencepiece --quiet\n\nimport os\nimport gc\nimport json\nimport math\nimport warnings\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nimport evaluate # Using evaluate library for metrics\nfrom sklearn.metrics import accuracy_score # Import accuracy score\n\n\n# Transformers imports\nfrom transformers import (\n    RobertaTokenizer,\n    RobertaForSequenceClassification,\n    BartTokenizer,\n    BartForConditionalGeneration, # Changed to BART\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainer, # Optional, we are using a custom loop\n    Seq2SeqTrainingArguments, # Optional\n    DataCollatorForSeq2Seq,\n    get_linear_schedule_with_warmup,\n    BertTokenizer,\n    BertModel,\n    BitsAndBytesConfig # For potential 4/8-bit loading\n)\n\n# PEFT imports\nfrom peft import (\n    get_peft_config,\n    get_peft_model,\n    get_peft_model_state_dict,\n    LoraConfig, # Changed to LoRA\n    TaskType,\n    PeftModel,\n    PeftConfig\n)\n\n# Evaluation imports\nimport nltk\n# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction # Using evaluate's BLEU\nfrom rouge_score import rouge_scorer # Using rouge_score library directly for Ea\nfrom bert_score import score as bert_score_metric # Renamed to avoid conflict\n\n# --- Basic Setup ---\nwarnings.filterwarnings(\"ignore\")\nnltk.download('punkt', quiet=True) # Download required NLTK data\n\n# Set environment variable for CUDA debugging if needed\n# os.environ['CUDA_LAUNCH_BLOCKING'] = '1' # Helps get clearer CUDA errors\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif device.type == 'cuda':\n    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"CUDA Version: {torch.version.cuda}\")\n    print(f\"PyTorch Version: {torch.__version__}\")\nelse:\n    print(\"CUDA not available, running on CPU.\")\n\n# Define paths for Kaggle (ensure your dataset is linked)\n# Ensure this path exists or your Kaggle kernel has the dataset attached\nBASE_PATH = '/kaggle/input/nlp-data'\nif not os.path.exists(BASE_PATH):\n     # Fallback for local testing if Kaggle path doesn't exist\n     print(f\"Warning: Kaggle path {BASE_PATH} not found. Using fallback './nlp-dataset'.\")\n     BASE_PATH = './nlp-dataset' # Adjust this to your local dataset path if needed\n     # You might need to create dummy files locally if needed for the script to run:\n     # os.makedirs(BASE_PATH, exist_ok=True)\n     # dummy_data = [{\"question\": \"q\", \"answers\": [\"a\"], \"labelled_summaries\": {\"INFORMATION_SUMMARY\": \"s\"}}]\n     # with open(os.path.join(BASE_PATH, 'train.json'), 'w') as f: json.dump(dummy_data, f)\n     # with open(os.path.join(BASE_PATH, 'valid.json'), 'w') as f: json.dump(dummy_data, f)\n     # with open(os.path.join(BASE_PATH, 'test.json'), 'w') as f: json.dump(dummy_data, f)\n\n\nOUTPUT_PATH = '/kaggle/working/'\n# Ensure output path exists (it should in Kaggle)\nos.makedirs(OUTPUT_PATH, exist_ok=True)\n\n# Create output directories\nGENERATED_DIR = os.path.join(OUTPUT_PATH, \"generated\")\nCHECKPOINTS_DIR = os.path.join(OUTPUT_PATH, \"checkpoints\")\nCLASSIFIER_CKPT_DIR = os.path.join(CHECKPOINTS_DIR, \"classifier\")\nSUMMARIZER_CKPT_DIR = os.path.join(CHECKPOINTS_DIR, \"summarizer\")\nPLOTS_DIR = os.path.join(OUTPUT_PATH, \"plots\")\n\nos.makedirs(GENERATED_DIR, exist_ok=True)\nos.makedirs(CHECKPOINTS_DIR, exist_ok=True)\nos.makedirs(CLASSIFIER_CKPT_DIR, exist_ok=True)\nos.makedirs(SUMMARIZER_CKPT_DIR, exist_ok=True)\nos.makedirs(PLOTS_DIR, exist_ok=True)\n\nprint(f\"Output base directory: {OUTPUT_PATH}\")\nprint(f\"Generated files directory: {GENERATED_DIR}\")\nprint(f\"Checkpoints directory: {CHECKPOINTS_DIR}\")\nprint(f\"Plots directory: {PLOTS_DIR}\")\n\n\n# Helper function for memory cleanup\ndef cleanup_memory():\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    print(\"Memory cleaned up.\")\n\n# %% [code]\n# --- ClassifierCustomDataset Implementation ---\n# Define perspective labels\nPERSPECTIVE_LABELS = {\n    \"INFORMATION\": 0,\n    \"SUGGESTION\": 1,\n    \"EXPERIENCE\": 2,\n    \"CAUSE\": 3,\n    \"QUESTION\": 4\n}\n# Reverse mapping for reference if needed\nLABEL_TO_PERSPECTIVE = {v: k for k, v in PERSPECTIVE_LABELS.items()}\n\nclass ClassifierCustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        # Store valid perspective names based on the mapping keys\n        self.valid_perspective_names = set(PERSPECTIVE_LABELS.keys())\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        answers = item.get('answers', [])\n        source_context = ' '.join([ans.replace('\\\\n', ' ').strip() for ans in answers if ans.strip()])\n        question = item.get('question', '').strip()\n        # Combine question and context for classification input\n        text_input = f\"Question: {question} Context: {source_context}\"\n\n        # Determine target perspective label\n        target_perspective_raw = \"INFORMATION\" # Default raw name\n        if item.get('labelled_summaries'):\n            first_perspective_raw = list(item['labelled_summaries'].keys())[0]\n            target_perspective_raw = first_perspective_raw\n\n        # Clean the name\n        cleaned_perspective = target_perspective_raw.replace('_SUMMARY', '')\n\n        # Ensure cleaned name is valid and get label\n        if cleaned_perspective in self.valid_perspective_names:\n            label = PERSPECTIVE_LABELS[cleaned_perspective]\n        else:\n            print(f\"Warning: Found unexpected perspective '{target_perspective_raw}' in classifier data, defaulting to INFORMATION label.\")\n            label = PERSPECTIVE_LABELS[\"INFORMATION\"] # Fallback\n\n        # Tokenize text input\n        inputs = self.tokenizer(\n            text_input,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n            \"label\": torch.tensor(label, dtype=torch.long) # Return label as tensor\n        }\n\n# %% [code]\n# --- DataLoader Implementations ---\n\n# Seq2Seq DataLoader (Remains mostly the same, uses perspective definitions from paper)\nclass SummarizationCustomDataset(Dataset):\n    def __init__(self, data, tokenizer, max_source_length=1024, max_target_length=256):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_source_length = max_source_length\n        self.max_target_length = max_target_length # Max length for summaries\n\n        # Define perspective attributes based on Table 3 of the paper\n        self.perspective_prompts = {\n            \"SUGGESTION\": {\n                \"defn\": \"Defined as advice or recommendations to assist users in making informed medical decisions, solving problems, or improving health issues.\",\n                \"start_with\": \"It is suggested\",\n                \"tone_attribute\": \"Advisory, Recommending\"\n            },\n            \"INFORMATION\": {\n                \"defn\": \"Defined as knowledge about diseases, disorders, and health-related facts, providing insights into symptoms and diagnosis.\",\n                \"start_with\": \"For information purposes\",\n                \"tone_attribute\": \"Informative, Educational\"\n            },\n            \"EXPERIENCE\": {\n                \"defn\": \"Defined as individual experiences, anecdotes, or firsthand insights related to health, medical treatments, medication usage, and coping strategies\",\n                \"start_with\": \"In user's experience\",\n                \"tone_attribute\": \"Personal, Narrative\"\n            },\n            \"QUESTION\": {\n                \"defn\": \"Defined as inquiry made for deeper understanding.\",\n                \"start_with\": \"It is inquired\",\n                \"tone_attribute\": \"Seeking Understanding\"\n            },\n            \"CAUSE\": {\n                \"defn\": \"Defined as reasons responsible for the occurrence of a particular medical condition, symptom, or disease\",\n                \"start_with\": \"Some of the causes\",\n                \"tone_attribute\": \"Explanatory, Causal\"\n            }\n            # --- FIX: Add QUESTION definition if missing ---\n            # \"QUESTION\": {\n            #     \"defn\": \"Defined as inquiry made for deeper understanding.\", # Provide a definition\n            #     \"start_with\": \"It is asked\", # Define a starting phrase\n            #     \"tone_attribute\": \"Inquisitive, Seeking\" # Define a tone\n            # }\n        }\n        self.valid_perspective_names = set(self.perspective_prompts.keys()) # Store valid names\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n\n        # Concatenate all non-empty answers as the source context\n        answers = item.get('answers', [])\n        source_context = ' '.join([ans.replace('\\\\n', ' ').strip() for ans in answers if ans.strip()])\n        question = item.get('question', '').strip()\n\n        # Determine target perspective and summary\n        target_perspective = \"INFORMATION\" # Default cleaned name\n        target_summary = \"\"\n        if item.get('labelled_summaries'):\n            first_perspective_raw = list(item['labelled_summaries'].keys())[0]\n            target_summary = item['labelled_summaries'][first_perspective_raw]\n\n            # --- Clean the name ---\n            cleaned_perspective = first_perspective_raw.replace('_SUMMARY', '')\n            # --- Ensure cleaned name is valid ---\n            if cleaned_perspective in self.valid_perspective_names:\n                target_perspective = cleaned_perspective # Use the cleaned, valid name\n            else:\n                print(f\"Warning: Found unexpected perspective '{first_perspective_raw}' in summarizer data, defaulting to INFORMATION.\")\n                target_perspective = \"INFORMATION\" # Fallback to default\n        else:\n            # Handle case where there are no labelled summaries if needed\n             target_perspective = \"INFORMATION\" # Keep default\n\n        # Get perspective-specific prompt details using the cleaned name\n        prompt_details = self.perspective_prompts.get(target_perspective, self.perspective_prompts[\"INFORMATION\"])\n        defn = prompt_details['defn']\n        start_with = prompt_details['start_with']\n        tone_attribute = prompt_details['tone_attribute']\n\n        # Ensure target summary starts with the anchor text (as per paper's prompt logic)\n        # Only add if summary has content and doesn't already start with it (case-insensitive)\n        if target_summary and not target_summary.lower().startswith(start_with.lower()):\n            target_summary = start_with + \" \" + target_summary\n\n        # Construct the prompt using the template from Figure 2 / Table 3\n        task_prefix = (\n            f\"Summarize the following content according to Perspective: {target_perspective}; \"\n            f\"{target_perspective} Definition: {defn}; \"\n            f\"Begin Summary with: '{start_with}'; \"\n            f\"Tone of summary: {tone_attribute}; \"\n            f\"Content to summarize: {source_context}; \"\n            f\"Associated question: {question}\"\n        )\n\n        # Tokenize source (prompt)\n        inputs = self.tokenizer(\n            task_prefix,\n            padding=\"max_length\",\n            max_length=self.max_source_length,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        # Tokenize target (summary)\n        labels = self.tokenizer(\n            target_summary,\n            padding=\"max_length\",\n            max_length=self.max_target_length,\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        # For BART, labels should not be padded with -100, the model handles padding tokens\n        label_ids = labels[\"input_ids\"].squeeze(0)\n        # Replace tokenizer.pad_token_id with -100 for labels so they are ignored in loss\n        label_ids[label_ids == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n            \"labels\": label_ids,\n            \"perspective\": target_perspective, # Pass the CLEANED perspective name\n            \"target_summary_text\": target_summary # Pass text for validation/logging\n        }\n\n# DataLoader Creation Functions\ndef create_dataloader(dataset, batch_size, shuffle=True, num_workers=2):\n     # Reduce num_workers if CPU count is low\n    num_workers = min(num_workers, os.cpu_count() // 2 if os.cpu_count() else 1)\n    if num_workers < 1: num_workers = 1\n    # print(f\"Using {num_workers} workers for DataLoader.\")\n    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True)\n\ndef test_create_dataloader(test_dataset, test_batch_size):\n     num_workers = min(2, os.cpu_count() // 2 if os.cpu_count() else 1)\n     if num_workers < 1: num_workers = 1\n     return DataLoader(dataset=test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n\n# %% [code]\n# --- Classifier Training and Validation Functions ---\n\ndef classifier_validation(valid_dataloader, model, device):\n    print(\"Classifier validation processing...\")\n    model.eval()\n    valid_losses = []\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        progress_bar = tqdm(valid_dataloader, desc=\"Classifier Validation\")\n        for batch in progress_bar:\n            try:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['label'].to(device)\n\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                logits = outputs.logits\n\n                valid_losses.append(loss.item())\n                preds = torch.argmax(logits, dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                progress_bar.set_postfix({'val_loss': np.mean(valid_losses) if valid_losses else 0.0})\n            except Exception as e:\n                print(f\"\\nError during classifier validation batch: {e}\")\n                cleanup_memory() # Try to clean up memory if an error occurs\n                continue # Skip batch\n\n    valid_loss = np.mean(valid_losses) if len(valid_losses) > 0 else float('inf') # Return inf if no batches succeeded\n    accuracy = accuracy_score(all_labels, all_preds) if all_labels and all_preds else 0.0\n    print(f\"Classifier Validation Loss: {valid_loss:.4f}, Accuracy: {accuracy:.4f}\")\n    return valid_loss, accuracy\n\ndef train_classifier(train_data_path, valid_data_path, classifier_output_dir, # Changed output_path to specific dir\n                     train_batch_size=8, valid_batch_size=8,\n                     learning_rate=2e-5, warmup_ratio=0.1, epochs=3): # Reduced epochs\n\n    print(\"--- Starting Classifier Training ---\")\n    # Load data\n    try:\n        with open(train_data_path, 'r') as json_file:\n            train_data = json.load(json_file)\n        with open(valid_data_path, 'r') as json_file:\n            valid_data = json.load(json_file)\n        print(f\"Loaded {len(train_data)} training samples and {len(valid_data)} validation samples for classifier.\")\n    except Exception as e:\n        print(f\"Error loading classifier data: {e}\")\n        return None, None\n\n    # Ensure output directory exists (redundant if created globally, but safe)\n    os.makedirs(classifier_output_dir, exist_ok=True)\n    print(f\"Classifier checkpoints will be saved to: {classifier_output_dir}\")\n\n    best_val_loss = float('inf')\n    num_perspectives = len(PERSPECTIVE_LABELS) # Use the defined mapping\n\n    # Initialize model and tokenizer\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n    model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=num_perspectives)\n    model.to(device)\n\n    # Create datasets and dataloaders\n    train_dataset = ClassifierCustomDataset(train_data, tokenizer)\n    eval_dataset = ClassifierCustomDataset(valid_data, tokenizer)\n    train_dataloader = create_dataloader(train_dataset, train_batch_size, shuffle=True)\n    eval_dataloader = create_dataloader(eval_dataset, valid_batch_size, shuffle=False)\n\n    # Define optimizer and learning rate scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    total_steps = len(train_dataloader) * epochs\n    num_warmup_steps = int(total_steps * warmup_ratio)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n\n    # Training loop\n    for epoch in range(epochs):\n        model.train()\n        print(f\"\\n{'#'*25} Classifier Epoch: {epoch+1}/{epochs} {'#'*25}\")\n        train_losses = []\n        progress_bar = tqdm(train_dataloader, desc=f\"Classifier Epoch {epoch+1} Training\")\n\n        for batch in progress_bar:\n            try:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['label'].to(device)\n\n                optimizer.zero_grad()\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                if torch.isnan(loss):\n                    print(\"NaN loss detected in classifier training! Skipping batch.\")\n                    optimizer.zero_grad()\n                    continue\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n                optimizer.step()\n                scheduler.step()\n\n                train_losses.append(loss.item())\n                progress_bar.set_postfix({'train_loss': np.mean(train_losses[-50:]) if train_losses else 0.0})\n\n            except RuntimeError as e:\n                 if \"out of memory\" in str(e):\n                     print(f\"\\nCUDA OOM Error during classifier training. Skipping batch.\")\n                     cleanup_memory()\n                     optimizer.zero_grad()\n                     continue\n                 else:\n                     print(f\"\\nRuntime error during classifier training: {e}\")\n                     raise e\n            except Exception as e:\n                print(f\"\\nError during classifier training batch: {e}\")\n                raise e # Re-raise other errors\n\n        avg_train_loss = np.mean(train_losses) if train_losses else 0.0\n        print(f\"Classifier Epoch {epoch+1} Average Train Loss: {avg_train_loss:.4f}\")\n\n        # Validation\n        valid_loss, accuracy = classifier_validation(eval_dataloader, model, device)\n\n        # --- Save Best Checkpoint ---\n        if valid_loss < best_val_loss:\n            best_val_loss = valid_loss\n            print(f\"* New best validation loss: {best_val_loss:.4f}. Saving model to {classifier_output_dir}... *\")\n            try:\n                model.save_pretrained(classifier_output_dir)\n                tokenizer.save_pretrained(classifier_output_dir)\n                print(f\"Successfully saved classifier model and tokenizer to {classifier_output_dir}\")\n                # --- Verification ---\n                config_path = os.path.join(classifier_output_dir, \"config.json\")\n                model_path = os.path.join(classifier_output_dir, \"pytorch_model.bin\") # Or model.safetensors\n                tokenizer_path = os.path.join(classifier_output_dir, \"tokenizer_config.json\")\n                if os.path.exists(config_path) and (os.path.exists(model_path) or os.path.exists(model_path.replace(\".bin\",\".safetensors\"))) and os.path.exists(tokenizer_path):\n                     print(\"Classifier checkpoint files verified.\")\n                else:\n                     print(f\"!!! Warning: Checkpoint files verification failed in {classifier_output_dir} !!!\")\n                     if not os.path.exists(config_path): print(\" - Missing:\", config_path)\n                     if not (os.path.exists(model_path) or os.path.exists(model_path.replace(\".bin\",\".safetensors\"))): print(\" - Missing model file (.bin or .safetensors)\")\n                     if not os.path.exists(tokenizer_path): print(\" - Missing:\", tokenizer_path)\n\n            except Exception as e:\n                print(f\"!!! Error saving classifier checkpoint: {e} !!!\")\n        else:\n             print(f\"Validation loss ({valid_loss:.4f}) did not improve from best ({best_val_loss:.4f}). Not saving.\")\n\n    print(\"--- Classifier Training Finished ---\")\n    cleanup_memory()\n    # Return path to best model directory (where the last successful save occurred)\n    return classifier_output_dir # Return the directory path directly\n\n# %% [code]\n# --- Energy Model Initialization and Custom Loss ---\n\n# Global cache for energy models to avoid reloading if possible\nenergy_model_cache = {\n    \"bert_tokenizer\": None, \"bert_model\": None,\n    \"roberta_tokenizer\": None, \"roberta_model\": None,\n    \"device\": None\n}\n\ndef initialize_energy_models(classifier_checkpoint_path=None, device='cpu', force_reload=False):\n    global energy_model_cache\n    target_device = torch.device(device)\n\n    # Check if models are already loaded on the correct device\n    if not force_reload and \\\n       energy_model_cache[\"bert_model\"] is not None and \\\n       energy_model_cache[\"roberta_model\"] is not None and \\\n       energy_model_cache[\"device\"] == target_device:\n        print(f\"Using cached energy models on device: {target_device}\")\n        return energy_model_cache[\"bert_tokenizer\"], energy_model_cache[\"bert_model\"], \\\n               energy_model_cache[\"roberta_tokenizer\"], energy_model_cache[\"roberta_model\"]\n\n    print(f\"Initializing energy models for device: {target_device}\")\n    bert_tokenizer, bert_model, roberta_tokenizer, roberta_model = None, None, None, None\n\n    try:\n        # Clear cache before loading large models\n        cleanup_memory()\n\n        # --- BERT for Et ---\n        print(\"Loading BERT model (for Et)...\")\n        bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        bert_model = BertModel.from_pretrained('bert-base-uncased')\n        bert_model.to(target_device) # Move to target device\n        bert_model.eval()\n        print(f\"BERT model loaded on {bert_model.device}.\")\n\n        # --- RoBERTa Classifier for Ep ---\n        num_perspectives = len(PERSPECTIVE_LABELS)\n        print(\"Loading RoBERTa model (for Ep)...\")\n        if classifier_checkpoint_path and os.path.exists(classifier_checkpoint_path):\n            print(f\"Loading fine-tuned RoBERTa classifier from: {classifier_checkpoint_path}\")\n            # Verify checkpoint content before loading\n            config_path = os.path.join(classifier_checkpoint_path, \"config.json\")\n            model_path = os.path.join(classifier_checkpoint_path, \"pytorch_model.bin\") # Or model.safetensors\n            if not os.path.exists(config_path) or not (os.path.exists(model_path) or os.path.exists(model_path.replace(\".bin\",\".safetensors\"))):\n                 print(f\"!!! Error: Classifier checkpoint files missing in {classifier_checkpoint_path}. Loading base model instead.\")\n                 classifier_checkpoint_path = None # Force loading base model\n            else:\n                 roberta_tokenizer = RobertaTokenizer.from_pretrained(classifier_checkpoint_path)\n                 roberta_model = RobertaForSequenceClassification.from_pretrained(classifier_checkpoint_path, num_labels=num_perspectives)\n        else:\n             classifier_checkpoint_path = None # Ensure path is None if not used\n\n        if classifier_checkpoint_path is None: # Load base if checkpoint wasn't valid or provided\n            print(\"Warning: Fine-tuned classifier checkpoint not found or invalid. Loading base roberta-base.\")\n            print(\"         The Ep energy term will be based on the base model, which might be less effective.\")\n            roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n            roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_perspectives)\n\n        roberta_model.to(target_device) # Move to target device\n        roberta_model.eval()\n        print(f\"RoBERTa model loaded on {roberta_model.device}.\")\n\n        print(f\"Energy models initialized. BERT on: {bert_model.device}, RoBERTa on: {roberta_model.device}\")\n\n        # Update cache\n        energy_model_cache[\"bert_tokenizer\"] = bert_tokenizer\n        energy_model_cache[\"bert_model\"] = bert_model\n        energy_model_cache[\"roberta_tokenizer\"] = roberta_tokenizer\n        energy_model_cache[\"roberta_model\"] = roberta_model\n        energy_model_cache[\"device\"] = target_device\n\n        return bert_tokenizer, bert_model, roberta_tokenizer, roberta_model\n\n    except Exception as e:\n        print(f\"FATAL Error initializing energy models: {e}\")\n        import traceback\n        traceback.print_exc()\n        # Clear cache on failure\n        energy_model_cache = {k: None for k in energy_model_cache}\n        return None, None, None, None\n\n\n# --- Define Energy Functions (Ep, Ea, Et) based on paper ---\n\n# Perspective Energy (Ep) - using RoBERTa classifier\ndef calculate_Ep(text, tokenizer, model, device):\n    model.eval() # Ensure model is in eval mode\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=\"max_length\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # Ensure logits are on CPU for numpy conversion if needed, but softmax is fine on GPU\n        probabilities = torch.softmax(logits, dim=-1).squeeze() # Get probabilities for the classes\n    # Order needs to match PERSPECTIVE_LABELS: INFO, SUGG, EXP, CAUSE, QUES\n    # Lower energy is better -> use -log(prob)\n    log_probs = torch.log(probabilities + 1e-9) # Add epsilon to avoid log(0)\n    # Return energy for each perspective based on its index in PERSPECTIVE_LABELS\n    energies = {}\n    for perspective, index in PERSPECTIVE_LABELS.items():\n        energies[perspective] = -log_probs[index].item()\n    return energies\n\n\n# Anchor Energy (Ea) - ROUGE-1 between start of summary and anchor text\ndef calculate_Ea(generated_text, expected_anchor, scorer, device): # device param kept for consistency, but not used here\n     # Get first few words of generated text matching length of anchor\n     num_anchor_tokens = len(expected_anchor.split())\n     # Handle edge case of very short generated text\n     start_of_generated = \" \".join(generated_text.split()[:num_anchor_tokens])\n\n     if not start_of_generated or not expected_anchor:\n         return 1.0 # Max energy (bad score) if empty\n\n     # Calculate ROUGE-1 F1 score\n     scores = scorer.score(target=expected_anchor, prediction=start_of_generated)\n     rouge1_f1 = scores['rouge1'].fmeasure\n\n     # Lower energy is better, so return 1 - F1 score\n     energy = 1.0 - rouge1_f1\n     return energy\n\n\n# Tone Energy (Et) - Cosine similarity between summary embedding and tone keyword embeddings\nTONE_KEYWORDS = {\n    \"INFORMATION\": [\"factual\", \"informative\", \"educational\", \"objective\", \"knowledge\"],\n    \"SUGGESTION\": [\"advice\", \"recommend\", \"suggest\", \"should\", \"consider\"],\n    \"EXPERIENCE\": [\"personal\", \"narrative\", \"my experience\", \"I felt\", \"anecdote\"],\n    \"CAUSE\": [\"reason\", \"due to\", \"caused by\", \"explanation\", \"origin\"],\n    \"QUESTION\": [\"inquiry\", \"wondering\", \"question\", \"clarify\", \"understand\"]\n}\n# Cache for keyword embeddings\nkeyword_embedding_cache = {}\n\ndef get_bert_embedding(text, tokenizer, model, device):\n    model.eval()\n    # Batch process if text is a list? For now, single text\n    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device) # Use padding=True for single text\n    with torch.no_grad():\n        outputs = model(**inputs)\n        # Use mean pooling of last hidden state\n        attention_mask = inputs['attention_mask']\n        mask_expanded = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(outputs.last_hidden_state * mask_expanded, 1)\n        sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n        embedding = (sum_embeddings / sum_mask).squeeze(0) # Squeeze batch dim\n    return embedding\n\ndef calculate_Et(generated_text, perspective, bert_tokenizer, bert_model, device):\n    global keyword_embedding_cache\n    if not generated_text:\n        return 1.0 # Max energy if empty\n\n    keywords = TONE_KEYWORDS.get(perspective, [])\n    if not keywords:\n        return 0.5 # Default neutral energy if no keywords defined\n\n    # Get summary embedding\n    summary_embedding = get_bert_embedding(generated_text, bert_tokenizer, bert_model, device)\n    if summary_embedding is None: return 1.0 # Handle potential embedding error\n\n    # Get keyword embeddings (use cache)\n    cache_key = (perspective, str(bert_model.device)) # Include device in cache key\n    if cache_key in keyword_embedding_cache:\n        avg_keyword_embedding = keyword_embedding_cache[cache_key]\n    else:\n        keyword_embeddings = []\n        for keyword in keywords:\n            emb = get_bert_embedding(keyword, bert_tokenizer, bert_model, device)\n            if emb is not None:\n                 keyword_embeddings.append(emb)\n\n        if not keyword_embeddings:\n             return 0.5 # Default if keywords couldn't be embedded\n        avg_keyword_embedding = torch.stack(keyword_embeddings).mean(dim=0)\n        keyword_embedding_cache[cache_key] = avg_keyword_embedding # Store in cache\n\n    # Calculate cosine similarity\n    cos_sim = torch.nn.functional.cosine_similarity(summary_embedding, avg_keyword_embedding, dim=0)\n\n    # Lower energy is better, so return 1 - similarity\n    energy = 1.0 - cos_sim.item()\n    return max(0.0, energy) # Ensure energy is not negative\n\n# --- Combined Custom Loss Calculation ---\nperspective_details = SummarizationCustomDataset([], None).perspective_prompts # Get definitions for Ea anchor text\nrouge_calc_scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True) # Initialize rouge scorer for Ea\n\ndef compute_custom_loss(decoded_target_summary, # Use decoded target labels\n                      target_perspective, # Target perspective string\n                      tokenizer, # Main tokenizer (BART) - NOT USED here, but kept for potential future use\n                      bert_tokenizer, bert_model,\n                      roberta_tokenizer, roberta_model,\n                      device):\n    \"\"\"\n    Calculates the energy-based loss based on the paper's components (Ep, Ea, Et).\n    Operates on the DECODED TARGET SUMMARY for stability during training.\n    \"\"\"\n    current_target_device = torch.device(device)\n    if not decoded_target_summary.strip() or target_perspective not in perspective_details:\n        # print(f\"Warning: Skipping custom loss for empty summary or invalid perspective '{target_perspective}'.\")\n        return torch.tensor(0.0, device=current_target_device, requires_grad=False)\n\n    try:\n        # --- Ensure Energy Models are on Correct Device ---\n        # This check might be less critical if initialize_energy_models handles it robustly\n        if bert_model.device != current_target_device: bert_model.to(current_target_device)\n        if roberta_model.device != current_target_device: roberta_model.to(current_target_device)\n\n        # --- Calculate Energy Components ---\n        with torch.no_grad(): # No gradients needed through energy models themselves\n            # Ep: Perspective Score\n            Ep_dict = calculate_Ep(decoded_target_summary, roberta_tokenizer, roberta_model, current_target_device)\n            if not Ep_dict: # Handle potential calculation failure\n                 print(\"Warning: Ep calculation failed.\")\n                 return torch.tensor(0.0, device=current_target_device, requires_grad=False)\n\n            # Ea: Anchor Score\n            expected_anchor = perspective_details[target_perspective]['start_with']\n            Ea_val = calculate_Ea(decoded_target_summary, expected_anchor, rouge_calc_scorer, current_target_device)\n            Ea_dict = {p: 1.0 for p in perspective_details} # Default high energy\n            Ea_dict[target_perspective] = Ea_val # Set energy for the target perspective\n\n            # Et: Tone Score\n            Et_val = calculate_Et(decoded_target_summary, target_perspective, bert_tokenizer, bert_model, current_target_device)\n            Et_dict = {p: 0.5 for p in perspective_details} # Default neutral energy\n            Et_dict[target_perspective] = Et_val # Set energy for the target perspective\n\n        # --- Combine Energies (Linear Combination per Perspective) ---\n        alpha1 = 0.5 # Weight for Ep\n        alpha2 = 0.3 # Weight for Ea\n        alpha3 = 0.2 # Weight for Et\n\n        E_X = {}\n        perspectives_ordered = list(perspective_details.keys()) # Ensure consistent order\n        for p in perspectives_ordered:\n             ep_val = Ep_dict.get(p)\n             if ep_val is None: # Check if perspective was found in Ep_dict\n                 print(f\"Warning: Perspective '{p}' not found in Ep calculation results. Assigning high energy.\")\n                 ep_val = 10.0 # Assign high energy if missing\n             E_X[p] = (alpha1 * ep_val +\n                       alpha2 * Ea_dict.get(p, 1.0) +\n                       alpha3 * Et_dict.get(p, 0.5))\n\n        # --- Calculate Probabilities using Boltzmann distribution ---\n        try:\n             # Use log-sum-exp trick for numerical stability\n             energies_tensor = torch.tensor([E_X[k] for k in perspectives_ordered], device=current_target_device)\n             log_probs = torch.nn.functional.log_softmax(-energies_tensor, dim=0) # Note the minus sign for energy\n        except Exception as e:\n            print(f\"Error during log_softmax calculation: {e}\")\n            print(\"E_X values:\", E_X)\n            return torch.tensor(0.0, device=current_target_device, requires_grad=False)\n\n\n        # --- Calculate Cross-Entropy Loss (equivalent to Negative Log Likelihood) ---\n        target_idx = perspectives_ordered.index(target_perspective)\n        perspective_loss = -log_probs[target_idx] # NLL = -log(P(correct_class))\n\n        # Check for NaN/Inf\n        if torch.isnan(perspective_loss) or torch.isinf(perspective_loss):\n             print(\"Warning: NaN/Inf detected in custom perspective loss!\")\n             # print(\"Input summary:\", decoded_target_summary)\n             # print(\"Target perspective:\", target_perspective)\n             # print(\"Ep:\", Ep_dict)\n             # print(\"Ea:\", Ea_dict)\n             # print(\"Et:\", Et_dict)\n             # print(\"E_X:\", E_X)\n             # print(\"log_probs:\", log_probs)\n             return torch.tensor(0.0, device=current_target_device, requires_grad=False)\n\n        # This loss acts as a regularizer based on properties of the target summary.\n        # It doesn't directly use the main model's gradients for this part.\n        # Detach it so gradients don't flow back through the energy models.\n        return perspective_loss.detach()\n\n    except Exception as e:\n        print(f\"Error in compute_custom_loss for perspective {target_perspective}: {e}\")\n        import traceback\n        traceback.print_exc()\n        return torch.tensor(0.0, device=current_target_device, requires_grad=False)\n\n\n# %% [code]\n# --- Summarization Model Training and Validation ---\n\ndef seq2seq_validation(eval_dataloader, model, tokenizer, device):\n    print(\"Summarizer validation processing...\")\n    model.eval()\n    eval_losses = []\n    all_preds = []\n    all_refs = []\n    rouge_metric = evaluate.load('rouge', quiet=True)\n\n    with torch.no_grad():\n        progress_bar = tqdm(eval_dataloader, desc=\"Summarizer Validation\")\n        for batch in progress_bar:\n            try:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                target_texts = batch['target_summary_text'] # Get reference text\n\n                # Calculate standard CE Loss for validation\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                if not torch.isnan(loss):\n                    eval_losses.append(loss.item())\n\n                # Generate summaries for ROUGE calculation\n                # Make sure model is on the correct device for generation\n                if next(model.parameters()).device != torch.device(device):\n                    model.to(device)\n\n                generated_ids = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_length=256, # Match max_target_length\n                    num_beams=4,\n                    early_stopping=True,\n                    # pad_token_id=tokenizer.pad_token_id, # Ensure pad token is set\n                    # eos_token_id=tokenizer.eos_token_id  # Ensure EOS token is set\n                )\n                # Move generated_ids to CPU before decoding if they are on GPU\n                preds = tokenizer.batch_decode(generated_ids.cpu(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                all_preds.extend(preds)\n                all_refs.extend(target_texts) # Add the ground truth summaries\n\n                progress_bar.set_postfix({'val_loss': np.mean(eval_losses) if eval_losses else 0.0})\n            except RuntimeError as e:\n                 if \"out of memory\" in str(e):\n                     print(f\"\\nCUDA OOM Error during summarizer validation. Skipping batch.\")\n                     cleanup_memory()\n                     continue\n                 else:\n                     print(f\"\\nRuntime error during summarizer validation: {e}\")\n                     # Don't raise, try to continue validation if possible\n            except Exception as e:\n                print(f\"\\nError during summarizer validation batch: {e}\")\n                # Don't raise, try to continue validation\n\n\n    avg_eval_loss = np.mean(eval_losses) if eval_losses else float('inf') # Return inf if validation failed\n\n    # Calculate ROUGE score using evaluate library\n    rouge_l_f1 = 0.0\n    if all_preds and all_refs:\n        try:\n            # Filter out empty predictions/references which can cause issues\n            filtered_preds = [p for p, r in zip(all_preds, all_refs) if p and r]\n            filtered_refs = [r for p, r in zip(all_preds, all_refs) if p and r]\n            if filtered_preds:\n                 results = rouge_metric.compute(predictions=filtered_preds, references=filtered_refs)\n                 rouge_l_f1 = results.get('rougeLsum', 0.0) # Use rougeLsum for summaries\n            else:\n                 print(\"Warning: No valid prediction/reference pairs for ROUGE calculation.\")\n        except Exception as e:\n            print(f\"Error calculating ROUGE score during validation: {e}\")\n    else:\n         print(\"Warning: No predictions or references generated during validation.\")\n\n\n    print(f\"Summarizer Validation CE Loss: {avg_eval_loss:.4f}, ROUGE-Lsum F1: {rouge_l_f1:.4f}\")\n    cleanup_memory()\n    # Use CE loss for selecting best model, but ROUGE is also informative\n    return avg_eval_loss, rouge_l_f1\n\n\n# Modified train_seq2seq function\ndef train_seq2seq(train_data_path, valid_data_path, model_name, summarizer_output_dir, # Changed output_path to specific dir\n                  classifier_checkpoint_path, # Path to the trained RoBERTa classifier\n                  train_batch_size=2, # Small batch size for BART-large\n                  gradient_accumulation_steps=8, # Effective batch size = 2 * 8 = 16\n                  valid_batch_size=4,\n                  learning_rate=5e-5, # Standard LR for fine-tuning\n                  lora_r=16, lora_alpha=32, lora_dropout=0.05, # LoRA config\n                  lambda_perspective=0.1, # Weight for the custom energy loss\n                  warmup_ratio=0.1, epochs=3, # Reduced epochs\n                  use_energy_loss=True): # Flag to enable/disable custom loss\n\n    print(f\"--- Starting Summarizer Training (Model: {model_name}, PEFT: LoRA) ---\")\n    global target_device # Make target_device accessible for energy model checks\n    target_device = device # Assign global device based on initial setup\n\n    # --- Load data ---\n    try:\n        with open(train_data_path, 'r') as json_file:\n            train_data = json.load(json_file)\n        with open(valid_data_path, 'r') as json_file:\n            valid_data = json.load(json_file)\n        print(f\"Loaded {len(train_data)} training samples and {len(valid_data)} validation samples for summarizer.\")\n    except Exception as e:\n        print(f\"Error loading summarizer data: {e}\")\n        return None, None, []\n\n\n    # --- Initialize Energy Models ---\n    bert_tokenizer, bert_model, roberta_tokenizer, roberta_model = None, None, None, None\n    if use_energy_loss:\n        print(\"Initializing energy models for custom loss...\")\n        bert_tokenizer, bert_model, roberta_tokenizer, roberta_model = initialize_energy_models(\n            classifier_checkpoint_path, device=device, force_reload=True # Force reload for training context\n        )\n        if bert_model is None or roberta_model is None:\n            print(\"Warning: Failed to initialize one or more energy models. Disabling energy loss.\")\n            use_energy_loss = False\n        else:\n            print(\"Energy models initialized successfully.\")\n\n    # --- Initialize Summarization Model (BART + LoRA) ---\n    model = None\n    tokenizer = None\n    try:\n        print(f\"Loading base model: {model_name}\")\n        # Load base model (consider 4/8 bit if needed)\n        # quantization_config = BitsAndBytesConfig(load_in_8bit=True) # Example for 8-bit\n        # model = BartForConditionalGeneration.from_pretrained(model_name, quantization_config=quantization_config)\n        model = BartForConditionalGeneration.from_pretrained(model_name)\n        tokenizer = BartTokenizer.from_pretrained(model_name)\n        print(\"Base model loaded.\")\n\n        # Configure LoRA\n        lora_config = LoraConfig(\n            r=lora_r,\n            lora_alpha=lora_alpha,\n            target_modules=[\"q_proj\", \"v_proj\"], # Target modules for BART attention\n            lora_dropout=lora_dropout,\n            bias=\"none\",\n            task_type=TaskType.SEQ_2_SEQ_LM\n        )\n\n        # Apply LoRA PEFT to the model\n        model = get_peft_model(model, lora_config) # model is now the PeftModel\n        print(\"Applied LoRA configuration.\")\n        model.print_trainable_parameters()\n\n        # Move model to device *after* PEFT application\n        print(f\"Moving main model to {device}...\")\n        model.to(device)\n        print(f\"Main model is now on device: {next(model.parameters()).device}\")\n\n    except Exception as e:\n        print(f\"Error initializing summarization model: {e}\")\n        import traceback\n        traceback.print_exc()\n        cleanup_memory()\n        return None, None, []\n\n    # --- DataLoaders and Optimizer ---\n    train_dataset = SummarizationCustomDataset(train_data, tokenizer)\n    eval_dataset = SummarizationCustomDataset(valid_data, tokenizer)\n    train_dataloader = create_dataloader(train_dataset, train_batch_size, shuffle=True)\n    eval_dataloader = create_dataloader(eval_dataset, valid_batch_size, shuffle=False)\n\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate) # Optimize only trainable params\n    total_steps = math.ceil(len(train_dataloader) / gradient_accumulation_steps) * epochs\n    num_warmup_steps = int(total_steps * warmup_ratio)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n\n    # --- Training Loop ---\n    # Ensure output directory exists\n    os.makedirs(summarizer_output_dir, exist_ok=True)\n    print(f\"Summarizer checkpoints will be saved to: {summarizer_output_dir}\")\n\n    best_val_loss = float('inf')\n    train_losses_history = []\n    global_step = 0\n\n    for epoch in range(epochs):\n        model.train()\n        # Ensure energy models are also in eval mode if used\n        if use_energy_loss and bert_model and roberta_model:\n            bert_model.eval()\n            roberta_model.eval()\n\n        print(f\"\\n{'#'*25} Summarizer Epoch: {epoch+1}/{epochs} {'#'*25}\")\n        epoch_train_losses = []\n        epoch_ce_losses = []\n        epoch_p_losses = []\n        progress_bar = tqdm(train_dataloader, desc=f\"Summarizer Epoch {epoch+1} Training\")\n\n        optimizer.zero_grad() # Zero gradients at the start of the epoch accumulation cycle\n\n        for step, batch in enumerate(progress_bar):\n            try:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                perspectives = batch['perspective'] # List of perspectives\n                target_summaries = batch['target_summary_text'] # List of target texts\n\n                # --- Standard Forward Pass ---\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                ce_loss = outputs.loss\n\n                if torch.isnan(ce_loss):\n                    print(f\"NaN detected in CE loss at step {step}. Skipping batch.\")\n                    # optimizer.zero_grad() # Clear potentially bad gradients from this batch\n                    cleanup_memory()\n                    continue # Skip this batch entirely\n\n                # --- Calculate Custom Perspective Loss (if enabled) ---\n                perspective_loss = torch.tensor(0.0, device=device)\n                if use_energy_loss and bert_model is not None and roberta_model is not None:\n                    batch_perspective_loss = []\n                    for i in range(len(perspectives)): # Process each item in batch individually\n                         # Check device consistency before calling custom loss\n                         if bert_model.device != target_device: bert_model.to(target_device)\n                         if roberta_model.device != target_device: roberta_model.to(target_device)\n\n                         loss_item = compute_custom_loss(\n                             target_summaries[i], perspectives[i],\n                             tokenizer, bert_tokenizer, bert_model,\n                             roberta_tokenizer, roberta_model, device\n                         )\n                         # Ensure loss_item is a tensor before appending\n                         if isinstance(loss_item, torch.Tensor):\n                             batch_perspective_loss.append(loss_item)\n                         else: # Handle cases where compute_custom_loss might return non-tensor on error\n                              batch_perspective_loss.append(torch.tensor(0.0, device=device))\n\n\n                    if batch_perspective_loss:\n                        # Stack and calculate mean, ensuring all tensors are on the correct device\n                        valid_losses = [l.to(device) for l in batch_perspective_loss if torch.isfinite(l)]\n                        if valid_losses:\n                            perspective_loss = torch.mean(torch.stack(valid_losses))\n                        else:\n                            perspective_loss = torch.tensor(0.0, device=device) # Handle case where all items failed\n\n                    if torch.isnan(perspective_loss):\n                         print(f\"NaN detected in Perspective loss at step {step}. Setting to 0.\")\n                         perspective_loss = torch.tensor(0.0, device=device)\n\n\n                # --- Combine Losses ---\n                total_loss = ce_loss + lambda_perspective * perspective_loss\n\n                # --- Gradient Accumulation and Backward Pass ---\n                scaled_loss = total_loss / gradient_accumulation_steps\n                if torch.isnan(scaled_loss):\n                     print(f\"NaN detected in final scaled loss at step {step}. Skipping backward pass for this batch.\")\n                     # optimizer.zero_grad() # Ensure grads are zeroed if skipping optimizer step\n                     cleanup_memory()\n                     continue\n\n                scaled_loss.backward()\n\n                epoch_train_losses.append(total_loss.item()) # Log unscaled loss\n                epoch_ce_losses.append(ce_loss.item())\n                epoch_p_losses.append(perspective_loss.item())\n\n                # --- Optimizer Step ---\n                if (step + 1) % gradient_accumulation_steps == 0 or (step + 1 == len(train_dataloader)):\n                    # Unscale gradients before clipping/stepping (if using GradScaler, otherwise skip)\n                    # scaler.unscale_(optimizer) # If using GradScaler\n\n                    torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 1.0)\n\n                    # scaler.step(optimizer) # If using GradScaler\n                    # scaler.update() # If using GradScaler\n                    optimizer.step() # Use this if not using GradScaler\n\n                    scheduler.step()\n                    optimizer.zero_grad() # Zero gradients *after* optimizer step\n                    global_step += 1\n\n                    # Update progress bar display (more stable averages)\n                    avg_loss = np.mean(epoch_train_losses[-50*gradient_accumulation_steps:]) if epoch_train_losses else 0.0\n                    avg_ce_loss = np.mean(epoch_ce_losses[-50*gradient_accumulation_steps:]) if epoch_ce_losses else 0.0\n                    avg_p_loss = np.mean(epoch_p_losses[-50*gradient_accumulation_steps:]) if epoch_p_losses else 0.0\n                    progress_bar.set_postfix({\n                        'loss': f\"{total_loss.item():.4f}\",\n                        'avg_loss': f\"{avg_loss:.4f}\",\n                        'avg_CE': f\"{avg_ce_loss:.4f}\",\n                        'avg_P': f\"{avg_p_loss:.4f}\",\n                        'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n                    })\n\n                # Optional: Periodic cleanup\n                if global_step % 100 == 0:\n                    cleanup_memory()\n\n            except RuntimeError as e:\n                 if \"out of memory\" in str(e):\n                     print(f\"\\nCUDA OOM Error at step {step}. Skipping batch.\")\n                     cleanup_memory()\n                     optimizer.zero_grad() # Important to clear potentially bad grads\n                     continue\n                 else:\n                     print(f\"\\nRuntime error in training batch {step}: {e}\")\n                     # Consider stopping if a non-OOM runtime error occurs\n                     raise e\n            except Exception as e:\n                print(f\"\\nError in training batch {step}: {e}\")\n                import traceback\n                traceback.print_exc()\n                # Consider stopping on other exceptions\n                raise e\n\n\n        # --- End of Epoch ---\n        avg_epoch_train_loss = np.mean(epoch_train_losses) if epoch_train_losses else float('inf')\n        train_losses_history.append(avg_epoch_train_loss)\n        print(f\"Summarizer Epoch {epoch+1} Average Train Loss: {avg_epoch_train_loss:.4f}\")\n\n        # --- Validation ---\n        valid_loss, valid_rouge = seq2seq_validation(eval_dataloader, model, tokenizer, device)\n\n        # --- Save Best Model (based on validation CE loss) ---\n        if valid_loss < best_val_loss:\n            best_val_loss = valid_loss\n            print(f\"* New best validation loss: {best_val_loss:.4f}. Saving PEFT model to {summarizer_output_dir}... *\")\n            try:\n                model.save_pretrained(summarizer_output_dir) # Saves only LoRA weights + config\n                tokenizer.save_pretrained(summarizer_output_dir) # Save tokenizer config\n                print(f\"Successfully saved PEFT adapter and tokenizer to {summarizer_output_dir}\")\n\n                # --- Verification ---\n                adapter_model_path_bin = os.path.join(summarizer_output_dir, \"adapter_model.bin\")\n                adapter_model_path_safe = os.path.join(summarizer_output_dir, \"adapter_model.safetensors\")\n                adapter_config_path = os.path.join(summarizer_output_dir, \"adapter_config.json\")\n                tokenizer_config_path = os.path.join(summarizer_output_dir, \"tokenizer_config.json\")\n\n                adapter_model_exists = os.path.exists(adapter_model_path_bin) or os.path.exists(adapter_model_path_safe)\n\n                if adapter_model_exists and os.path.exists(adapter_config_path) and os.path.exists(tokenizer_config_path):\n                     print(\"PEFT adapter and tokenizer files verified.\")\n                else:\n                     print(f\"!!! Warning: PEFT checkpoint files verification failed in {summarizer_output_dir} !!!\")\n                     if not adapter_model_exists: print(f\" - Missing: adapter_model (.bin or .safetensors)\")\n                     if not os.path.exists(adapter_config_path): print(\" - Missing:\", adapter_config_path)\n                     if not os.path.exists(tokenizer_config_path): print(\" - Missing:\", tokenizer_config_path)\n\n            except Exception as e:\n                 print(f\"!!! Error saving PEFT adapter checkpoint: {e} !!!\")\n\n        else:\n            print(f\"Validation loss ({valid_loss:.4f}) did not improve from best ({best_val_loss:.4f}). Not saving.\")\n\n\n    print(\"--- Summarizer Training Finished ---\")\n    cleanup_memory()\n    # Return path to best model directory and loss history\n    # Important: return the *directory path*, not the model object itself\n    return summarizer_output_dir, tokenizer, train_losses_history\n\n\n# %% [code]\n# --- Inference Function ---\n\ndef inference(test_file, base_model_name, peft_ckpt_dir, output_csv_path, batch_size_test=4, max_source_length=1024, max_target_length=256):\n    print(\"--- Starting Inference ---\")\n    global target_device # Use the globally set device\n    # Load test data\n    try:\n        with open(test_file, 'r') as json_file:\n            test_data = json.load(json_file)\n        print(f\"Loaded {len(test_data)} test samples from {test_file}\")\n    except Exception as e:\n        print(f\"Error loading test data from {test_file}: {e}\")\n        return None\n\n    # Verify PEFT checkpoint directory and necessary files\n    print(f\"Checking PEFT checkpoint directory: {peft_ckpt_dir}\")\n    adapter_config_path = os.path.join(peft_ckpt_dir, \"adapter_config.json\")\n    adapter_model_path_bin = os.path.join(peft_ckpt_dir, \"adapter_model.bin\")\n    adapter_model_path_safe = os.path.join(peft_ckpt_dir, \"adapter_model.safetensors\")\n    if not os.path.exists(adapter_config_path):\n         print(f\"Error: adapter_config.json not found in {peft_ckpt_dir}\")\n         return None\n    if not (os.path.exists(adapter_model_path_bin) or os.path.exists(adapter_model_path_safe)):\n         print(f\"Error: adapter_model file (.bin or .safetensors) not found in {peft_ckpt_dir}\")\n         return None\n    print(\"PEFT checkpoint files seem present.\")\n\n\n    # Load base model and tokenizer\n    peft_model = None\n    tokenizer = None\n    try:\n        print(f\"Loading base model '{base_model_name}' for inference...\")\n        # Load base model potentially onto CPU first if memory is tight, then move PeftModel to GPU\n        base_model = BartForConditionalGeneration.from_pretrained(base_model_name)\n        tokenizer = BartTokenizer.from_pretrained(peft_ckpt_dir) # Load tokenizer from PEFT dir (if saved)\n        print(\"Base model and tokenizer loaded.\")\n\n        # Load PEFT adapter weights\n        print(f\"Loading PEFT adapter from: {peft_ckpt_dir}\")\n        peft_model = PeftModel.from_pretrained(base_model, peft_ckpt_dir, is_trainable=False)\n        print(\"PEFT adapter loaded.\")\n\n        # Move the combined model to the target device\n        print(f\"Moving PEFT model to device: {target_device}\")\n        peft_model.to(target_device)\n        peft_model.eval() # Set to evaluation mode\n        print(f\"PEFT model moved to {next(peft_model.parameters()).device} and set to eval mode.\")\n\n    except Exception as e:\n        print(f\"Error loading model for inference: {e}\")\n        import traceback\n        traceback.print_exc()\n        cleanup_memory()\n        return None\n\n    # Create test dataset and dataloader\n    # Use the same dataset class as training to ensure prompt consistency\n    test_dataset = SummarizationCustomDataset(test_data, tokenizer, max_source_length, max_target_length)\n    test_dataloader = test_create_dataloader(test_dataset, batch_size_test)\n\n    # Generate predictions\n    results = []\n    with torch.no_grad():\n        progress_bar = tqdm(test_dataloader, desc=\"Inference\")\n        # Keep track of original indices if dataloader shuffles (it shouldn't with test_create_dataloader)\n        current_idx = 0\n        for batch in progress_bar:\n            try:\n                input_ids = batch['input_ids'].to(target_device)\n                attention_mask = batch['attention_mask'].to(target_device)\n\n                # Check model device just before generation\n                if next(peft_model.parameters()).device != target_device:\n                     print(\"Warning: Model moved off device? Moving back...\")\n                     peft_model.to(target_device)\n\n                # Use the loaded PeftModel for generation\n                outputs = peft_model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    num_beams=5,\n                    max_new_tokens=max_target_length, # Use max_new_tokens for consistency with newer HF versions\n                    min_length=10, # Avoid very short summaries\n                    repetition_penalty=1.2,\n                    early_stopping=True,\n                    no_repeat_ngram_size=3,\n                    pad_token_id=tokenizer.pad_token_id, # Explicitly set pad token id\n                    eos_token_id=tokenizer.eos_token_id   # Explicitly set eos token id\n                )\n\n                # Decode generated summaries (move outputs to CPU first)\n                predictions = tokenizer.batch_decode(outputs.cpu(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n\n                # Store results (perspective, prediction, actual, source)\n                batch_size = len(batch['perspective'])\n                for i in range(batch_size):\n                     original_item_index = current_idx + i\n                     if original_item_index < len(test_data):\n                         item = test_data[original_item_index] # Get original item for full context\n                         answers = item.get('answers', [])\n                         source_context = ' '.join([ans.replace('\\\\n', ' ').strip() for ans in answers if ans.strip()])\n                         question = item.get('question', '').strip()\n\n                         results.append({\n                             'PERSPECTIVE': batch['perspective'][i],\n                             'PREDICTED': predictions[i],\n                             'ACTUAL_SUMMARY': batch['target_summary_text'][i],\n                             'SOURCE_QUESTION': question,\n                             'SOURCE_ANSWERS': source_context\n                         })\n                     else:\n                         print(f\"Warning: Index mismatch during inference result processing ({original_item_index} vs {len(test_data)})\")\n\n                current_idx += batch_size # Increment index tracker\n\n            except RuntimeError as e:\n                 if \"out of memory\" in str(e):\n                     print(f\"\\nCUDA OOM Error during inference. Skipping batch.\")\n                     cleanup_memory()\n                     continue\n                 else:\n                     print(f\"\\nRuntime error during inference: {e}\")\n                     # Don't raise, try to continue inference if possible\n            except Exception as e:\n                print(f\"\\nError during inference batch: {e}\")\n                # Don't raise, try to continue\n\n\n    # Save results to CSV\n    try:\n        results_df = pd.DataFrame(results)\n        # Ensure the directory exists before saving\n        os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n        results_df.to_csv(output_csv_path, index=False)\n        print(f\"Inference complete. Results saved to {output_csv_path}\")\n        # --- Verification ---\n        if os.path.exists(output_csv_path):\n            print(f\"Inference results CSV file verified at {output_csv_path}\")\n        else:\n            print(f\"!!! Warning: Inference results CSV file NOT found at {output_csv_path} after saving !!!\")\n    except Exception as e:\n         print(f\"!!! Error saving inference results to CSV: {e} !!!\")\n         results_df = None # Ensure df is None if saving failed\n\n    cleanup_memory()\n    return results_df\n\n\n# %% [code]\n# --- Evaluation Metrics ---\n\nclass EvaluationMetrics:\n    def __init__(self, predictions, references):\n        # Ensure lists are not empty and are of strings\n        self.predictions = [str(p) if p else \"\" for p in predictions] # Replace None/NaN with empty string\n        self.references = [str(r) if r else \"\" for r in references]\n\n        if not self.predictions or not self.references:\n             print(\"Warning: Predictions or References list is empty or became empty after cleaning.\")\n             self.valid_data = False\n        elif len(self.predictions) != len(self.references):\n             print(f\"Warning: Mismatch in prediction ({len(self.predictions)}) and reference ({len(self.references)}) counts.\")\n             # Optionally trim to the shorter length\n             min_len = min(len(self.predictions), len(self.references))\n             self.predictions = self.predictions[:min_len]\n             self.references = self.references[:min_len]\n             self.valid_data = bool(min_len)\n             print(f\"Trimmed to {min_len} pairs for evaluation.\")\n        else:\n            self.valid_data = True\n\n        # Load metrics only if data is valid\n        if self.valid_data:\n             self.rouge_metric = evaluate.load('rouge', quiet=True)\n             self.meteor_metric = evaluate.load('meteor', quiet=True)\n             self.bleu_metric = evaluate.load('bleu', quiet=True)\n             # BERTScore will be handled within its method due to potential slowness\n\n    def compute_rouge_score(self):\n        if not self.valid_data: return {}\n        # Filter empty strings again, as evaluate library might handle them differently\n        paired_preds = [p for p, r in zip(self.predictions, self.references) if p and r]\n        paired_refs = [r for p, r in zip(self.predictions, self.references) if p and r]\n        if not paired_preds: return {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0, \"rougeLsum\": 0}\n        try:\n            results = self.rouge_metric.compute(predictions=paired_preds, references=paired_refs)\n            return {\n                \"rouge1\": results.get('rouge1', 0.0) * 100,\n                \"rouge2\": results.get('rouge2', 0.0) * 100,\n                \"rougeL\": results.get('rougeL', 0.0) * 100,\n                \"rougeLsum\": results.get('rougeLsum', 0.0) * 100,\n            }\n        except Exception as e:\n            print(f\"Error computing ROUGE: {e}\")\n            return {}\n\n    def compute_meteor_score(self):\n        if not self.valid_data: return {}\n        paired_preds = [p for p, r in zip(self.predictions, self.references) if p and r]\n        paired_refs = [r for p, r in zip(self.predictions, self.references) if p and r]\n        if not paired_preds: return {\"meteor\": 0}\n        try:\n            # Meteor requires NLTK wordnet, download if needed\n            try:\n                 nltk.data.find('corpora/wordnet.zip')\n            except:\n                 nltk.download('wordnet', quiet=True)\n                 nltk.download('omw-1.4', quiet=True) # Also download Open Multilingual Wordnet\n\n            results = self.meteor_metric.compute(predictions=paired_preds, references=paired_refs)\n            return {\"meteor\": results.get('meteor', 0.0) * 100}\n        except Exception as e:\n            print(f\"Error computing METEOR: {e}\")\n            return {}\n\n    def compute_bleu_scores(self):\n        if not self.valid_data: return {}\n        paired_preds = [p for p, r in zip(self.predictions, self.references) if p and r]\n        paired_refs = [r for p, r in zip(self.predictions, self.references) if p and r]\n        if not paired_preds: return {\"bleu\": 0}\n        try:\n            # BLEU expects references as list of lists for multiple refs, but evaluate handles single refs\n            results = self.bleu_metric.compute(predictions=paired_preds, references=paired_refs)\n            return {\"bleu\": results.get('bleu', 0.0) * 100}\n        except Exception as e:\n            print(f\"Error computing BLEU: {e}\")\n            return {}\n\n    def compute_bertscore(self, lang=\"en\"):\n        if not self.valid_data: return {}\n        paired_preds = [p for p, r in zip(self.predictions, self.references) if p and r]\n        paired_refs = [r for p, r in zip(self.predictions, self.references) if p and r]\n        if not paired_preds: return {\"bertscore_precision\": 0, \"bertscore_recall\": 0, \"bertscore_f1\": 0}\n        try:\n            # BERTScore computation needs device context\n            with torch.no_grad(): # Ensure no grads during scoring\n                P, R, F1 = bert_score_metric(paired_preds, paired_refs, lang=lang, verbose=False, device=device)\n            return {\n                \"bertscore_precision\": P.mean().item() * 100,\n                \"bertscore_recall\": R.mean().item() * 100,\n                \"bertscore_f1\": F1.mean().item() * 100,\n            }\n        except Exception as e:\n            print(f\"Error computing BERTScore: {e}\")\n            # Attempt cleanup if it was an OOM error\n            if \"out of memory\" in str(e).lower():\n                cleanup_memory()\n            return {}\n\n    def evaluate_all(self):\n        \"\"\"Run all evaluation metrics and return a combined dictionary of results\"\"\"\n        if not self.valid_data:\n             print(\"Cannot evaluate: Invalid or empty prediction/reference data.\")\n             return {}\n\n        print(\"Calculating evaluation metrics...\")\n        results = {}\n        results.update(self.compute_rouge_score())\n        results.update(self.compute_meteor_score())\n        results.update(self.compute_bleu_scores())\n\n        print(\"Calculating BERTScore (this may take a moment)...\")\n        # Add BERTScore calculation here, ensuring device consistency\n        bertscore_results = self.compute_bertscore()\n        results.update(bertscore_results)\n        print(\"Evaluation metrics calculation complete.\")\n        return results\n\n# %% [code]\n# --- Main Execution ---\n\nif __name__ == \"__main__\": # Use this block if running as a script\n\n    # --- Configuration ---\n    TRAIN_CLASSIFIER = True # Set to False if you already have a trained classifier\n    TRAIN_SUMMARIZER = True # Set to False to skip summarizer training\n    RUN_INFERENCE = True   # Set to False to skip inference\n    RUN_EVALUATION = True  # Set to False to skip evaluation\n\n    CLASSIFIER_EPOCHS = 1 # Quick train for demo\n    SUMMARIZER_EPOCHS = 1 # Quick train for demo\n    SUMMARIZER_BASE_MODEL = \"facebook/bart-large\"\n    USE_ENERGY_LOSS_TRAINING = False # <<< DISABLE Energy Loss by default due to complexity/instability\n                                     # Set to True to experiment, ensure classifier is trained first\n\n    TRAIN_FILE = os.path.join(BASE_PATH, 'train.json')\n    VALID_FILE = os.path.join(BASE_PATH, 'valid.json')\n    TEST_FILE = os.path.join(BASE_PATH, 'test.json')\n\n    # --- Verify Data Files Exist ---\n    print(\"Checking for data files...\")\n    for f_path in [TRAIN_FILE, VALID_FILE, TEST_FILE]:\n        if not os.path.exists(f_path):\n            print(f\"!!! FATAL ERROR: Data file not found at {f_path} !!!\")\n            print(\"Please ensure the dataset is correctly linked/placed.\")\n            # exit() # Exit if data is missing\n            # For testing without data, comment out exit() and expect downstream errors\n        else:\n             print(f\"Found: {f_path}\")\n\n    # Use the predefined checkpoint directory paths\n    classifier_checkpoint_dir = CLASSIFIER_CKPT_DIR\n    summarizer_checkpoint_dir = SUMMARIZER_CKPT_DIR\n\n    # --- 1. Train Classifier (RoBERTa for Ep) ---\n    classifier_trained_successfully = False\n    if TRAIN_CLASSIFIER:\n        print(\"\\n--- Initiating Classifier Training ---\")\n        # The function now takes the specific output directory\n        train_classifier(\n            train_data_path=TRAIN_FILE,\n            valid_data_path=VALID_FILE,\n            classifier_output_dir=classifier_checkpoint_dir, # Pass the defined path\n            epochs=CLASSIFIER_EPOCHS,\n            # Adjust batch sizes if needed based on GPU memory\n            train_batch_size=8,\n            valid_batch_size=16\n        )\n        # Check if training likely succeeded by verifying output files\n        config_path = os.path.join(classifier_checkpoint_dir, \"config.json\")\n        model_path = os.path.join(classifier_checkpoint_dir, \"pytorch_model.bin\") # Or model.safetensors\n        if os.path.exists(config_path) and (os.path.exists(model_path) or os.path.exists(model_path.replace(\".bin\",\".safetensors\"))):\n            print(f\"Classifier training finished. Checkpoint seems saved in {classifier_checkpoint_dir}\")\n            classifier_trained_successfully = True\n        else:\n            print(f\"Classifier training finished, but checkpoint files were NOT verified in {classifier_checkpoint_dir}. Energy loss might fail.\")\n            classifier_trained_successfully = False\n\n    elif not os.path.exists(os.path.join(classifier_checkpoint_dir, \"config.json\")):\n        print(f\"Classifier training skipped, and required config.json not found at {classifier_checkpoint_dir}.\")\n        if USE_ENERGY_LOSS_TRAINING:\n            print(\"Disabling energy loss because the required classifier checkpoint is missing.\")\n            USE_ENERGY_LOSS_TRAINING = False\n        classifier_trained_successfully = False\n    else:\n        print(f\"Skipping classifier training. Using existing checkpoint: {classifier_checkpoint_dir}\")\n        classifier_trained_successfully = True # Assume existing checkpoint is valid\n\n    # Ensure energy loss is disabled if classifier training failed or was skipped without a valid checkpoint\n    if USE_ENERGY_LOSS_TRAINING and not classifier_trained_successfully:\n        print(\"Warning: Disabling energy loss as a valid classifier checkpoint is not available.\")\n        USE_ENERGY_LOSS_TRAINING = False\n\n\n    # --- 2. Train Summarizer (BART + LoRA + Optional Energy Loss) ---\n    summarizer_tokenizer = None\n    summarizer_loss_history = []\n    summarizer_trained_successfully = False\n\n    if TRAIN_SUMMARIZER:\n        print(\"\\n--- Initiating Summarizer Training ---\")\n        # train_seq2seq now returns the directory path, tokenizer, and history\n        summarizer_saved_dir, summarizer_tokenizer, summarizer_loss_history = train_seq2seq(\n            train_data_path=TRAIN_FILE,\n            valid_data_path=VALID_FILE,\n            model_name=SUMMARIZER_BASE_MODEL,\n            summarizer_output_dir=summarizer_checkpoint_dir, # Pass the defined path\n            classifier_checkpoint_path=classifier_checkpoint_dir if USE_ENERGY_LOSS_TRAINING else None,\n            epochs=SUMMARIZER_EPOCHS,\n            use_energy_loss=USE_ENERGY_LOSS_TRAINING,\n            # Adjust batch sizes and grad accum based on GPU memory\n            train_batch_size=2,        # BART-Large needs small batches\n            gradient_accumulation_steps=16, # Effective batch size 32\n            valid_batch_size=4,\n            lambda_perspective=0.1 # Weight for custom loss if used\n        )\n        # Check if training likely succeeded by verifying output files\n        adapter_config_path = os.path.join(summarizer_checkpoint_dir, \"adapter_config.json\")\n        adapter_model_path_bin = os.path.join(summarizer_checkpoint_dir, \"adapter_model.bin\")\n        adapter_model_path_safe = os.path.join(summarizer_checkpoint_dir, \"adapter_model.safetensors\")\n        if os.path.exists(adapter_config_path) and (os.path.exists(adapter_model_path_bin) or os.path.exists(adapter_model_path_safe)):\n            print(f\"Summarizer training finished. PEFT Checkpoint seems saved in {summarizer_checkpoint_dir}\")\n            summarizer_trained_successfully = True\n        else:\n            print(f\"Summarizer training finished, but PEFT checkpoint files were NOT verified in {summarizer_checkpoint_dir}. Inference might fail.\")\n            summarizer_trained_successfully = False\n\n        # Plot training loss if history is available\n        if summarizer_loss_history:\n            try:\n                plt.figure(figsize=(10, 6))\n                # Filter out potential inf values before plotting\n                plot_loss_history = [l for l in summarizer_loss_history if np.isfinite(l)]\n                if plot_loss_history:\n                     plt.plot(range(1, len(plot_loss_history) + 1), plot_loss_history, marker='o')\n                     plt.title('Summarizer Training Loss per Epoch')\n                     plt.xlabel('Epoch')\n                     plt.ylabel('Average Loss')\n                     plt.grid(True)\n                     plot_path = os.path.join(PLOTS_DIR, \"summarizer_training_loss.png\")\n                     plt.savefig(plot_path)\n                     print(f\"Training loss plot saved to {plot_path}\")\n                     # plt.show() # Show plot in notebook if desired\n                     plt.close() # Close the plot to free memory\n                else:\n                     print(\"No finite loss values recorded to plot.\")\n            except Exception as e:\n                print(f\"Error plotting training loss: {e}\")\n\n    elif not os.path.exists(os.path.join(summarizer_checkpoint_dir, \"adapter_config.json\")):\n        print(f\"Summarizer training skipped, and required adapter_config.json not found at {summarizer_checkpoint_dir}.\")\n        RUN_INFERENCE = False # Disable inference if no model\n        RUN_EVALUATION = False\n        summarizer_trained_successfully = False\n    else:\n        print(f\"Skipping summarizer training. Using existing checkpoint: {summarizer_checkpoint_dir}\")\n        # Need to load the tokenizer if skipping training but running inference\n        try:\n             summarizer_tokenizer = BartTokenizer.from_pretrained(summarizer_checkpoint_dir)\n             summarizer_trained_successfully = True # Assume valid checkpoint if tokenizer loads\n        except Exception as e:\n             print(f\"Could not load tokenizer from {summarizer_checkpoint_dir}. Trying base model tokenizer. Error: {e}\")\n             try:\n                 summarizer_tokenizer = BartTokenizer.from_pretrained(SUMMARIZER_BASE_MODEL)\n                 summarizer_trained_successfully = True # Still assume checkpoint might be usable\n             except Exception as e2:\n                  print(f\"Could not load base model tokenizer either: {e2}\")\n                  summarizer_trained_successfully = False\n                  RUN_INFERENCE = False\n                  RUN_EVALUATION = False\n\n\n    # --- 3. Run Inference ---\n    inference_results_df = None\n    generated_csv_path = os.path.join(GENERATED_DIR, \"bart_lora_generated_results.csv\")\n\n    if RUN_INFERENCE:\n        if not summarizer_trained_successfully:\n             print(f\"Cannot run inference: Summarizer model checkpoint is missing or invalid in {summarizer_checkpoint_dir}.\")\n        else:\n            print(\"\\n--- Initiating Inference ---\")\n            inference_results_df = inference(\n                 test_file=TEST_FILE,\n                 base_model_name=SUMMARIZER_BASE_MODEL,\n                 peft_ckpt_dir=summarizer_checkpoint_dir, # Use the specific path\n                 output_csv_path=generated_csv_path,\n                 batch_size_test=8 # Adjust as needed based on GPU memory for inference\n            )\n            if inference_results_df is not None:\n                  print(\"Sample Inference Results:\")\n                  print(inference_results_df.head())\n\n\n    # --- 4. Run Evaluation ---\n    if RUN_EVALUATION:\n        if inference_results_df is None:\n             # Try to load from CSV if inference wasn't run in this session but file exists\n             if os.path.exists(generated_csv_path):\n                  print(f\"\\n--- Loading previous inference results for Evaluation from {generated_csv_path} ---\")\n                  try:\n                      inference_results_df = pd.read_csv(generated_csv_path)\n                  except Exception as e:\n                       print(f\"Error loading inference results CSV: {e}\")\n                       inference_results_df = None # Ensure it's None if loading fails\n             else:\n                  print(\"Cannot run evaluation: No inference results DataFrame available and CSV file not found.\")\n\n        if inference_results_df is not None:\n             print(\"\\n--- Initiating Evaluation ---\")\n             # Ensure columns exist and handle potential NaN values robustly\n             if 'PREDICTED' not in inference_results_df.columns or 'ACTUAL_SUMMARY' not in inference_results_df.columns:\n                  print(\"Error: Required columns ('PREDICTED', 'ACTUAL_SUMMARY') not found in inference results DataFrame.\")\n             else:\n                 predictions = inference_results_df['PREDICTED'].fillna('').astype(str).tolist()\n                 references = inference_results_df['ACTUAL_SUMMARY'].fillna('').astype(str).tolist()\n\n                 evaluator = EvaluationMetrics(predictions, references)\n                 all_scores = evaluator.evaluate_all()\n\n                 print(\"\\n--- Evaluation Scores ---\")\n                 print(json.dumps(all_scores, indent=2))\n\n                 # Save scores to a file\n                 scores_path = os.path.join(GENERATED_DIR, \"bart_lora_evaluation_scores.json\")\n                 try:\n                     # Ensure the directory exists\n                     os.makedirs(os.path.dirname(scores_path), exist_ok=True)\n                     with open(scores_path, 'w') as f:\n                          json.dump(all_scores, f, indent=2)\n                     print(f\"Evaluation scores saved to {scores_path}\")\n                     # --- Verification ---\n                     if os.path.exists(scores_path):\n                          print(f\"Evaluation scores JSON file verified at {scores_path}\")\n                     else:\n                          print(f\"!!! Warning: Evaluation scores JSON file NOT found at {scores_path} after saving !!!\")\n                 except Exception as e:\n                     print(f\"!!! Error saving evaluation scores: {e} !!!\")\n        else:\n             print(\"Skipping evaluation as inference results are missing or failed to load.\")\n\n    print(f\"\\n--- Pipeline Finished ---\")\n    print(f\"All outputs generated in: {OUTPUT_PATH}\")\n    print(f\"Checkpoints saved in: {CHECKPOINTS_DIR}\")\n    print(f\"Generated files (CSV, JSON) saved in: {GENERATED_DIR}\")\n    print(f\"Plots saved in: {PLOTS_DIR}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:12:38.397069Z","iopub.execute_input":"2025-04-15T15:12:38.397426Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nCUDA Device Name: Tesla T4\nCUDA Version: 12.4\nPyTorch Version: 2.5.1+cu124\nOutput base directory: /kaggle/working/\nGenerated files directory: /kaggle/working/generated\nCheckpoints directory: /kaggle/working/checkpoints\nPlots directory: /kaggle/working/plots\nChecking for data files...\nFound: /kaggle/input/nlp-data/train.json\nFound: /kaggle/input/nlp-data/valid.json\nFound: /kaggle/input/nlp-data/test.json\n\n--- Initiating Classifier Training ---\n--- Starting Classifier Training ---\nLoaded 2236 training samples and 959 validation samples for classifier.\nClassifier checkpoints will be saved to: /kaggle/working/checkpoints/classifier\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n######################### Classifier Epoch: 1/1 #########################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Classifier Epoch 1 Training:   0%|          | 0/280 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c96e1e0b57664a13a00a5daabfe73b92"}},"metadata":{}},{"name":"stdout","text":"Classifier Epoch 1 Average Train Loss: 1.1479\nClassifier validation processing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Classifier Validation:   0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f06aeb3fdbb471dbe524aaa10580318"}},"metadata":{}},{"name":"stdout","text":"Classifier Validation Loss: 1.0488, Accuracy: 0.6382\n* New best validation loss: 1.0488. Saving model to /kaggle/working/checkpoints/classifier... *\nSuccessfully saved classifier model and tokenizer to /kaggle/working/checkpoints/classifier\n!!! Warning: Checkpoint files verification failed in /kaggle/working/checkpoints/classifier !!!\n - Missing model file (.bin or .safetensors)\n--- Classifier Training Finished ---\nMemory cleaned up.\nClassifier training finished, but checkpoint files were NOT verified in /kaggle/working/checkpoints/classifier. Energy loss might fail.\n\n--- Initiating Summarizer Training ---\n--- Starting Summarizer Training (Model: facebook/bart-large, PEFT: LoRA) ---\nLoaded 2236 training samples and 959 validation samples for summarizer.\nLoading base model: facebook/bart-large\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bab7dea4d14b448ea9e36676fd40a78c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f7f6644d3e34e76a0b5833e50620f1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa42d0760534c69b3bbe0dc262cd99e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24b6b8ba79fa43b4bfc04d8a410604e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ca9ed6def74f8c9f2c45aa3a0d46e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9f7cf817dd94acd961d97231d857eb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0bbf0bda3943adb86b840b878eb590"}},"metadata":{}},{"name":"stdout","text":"Base model loaded.\nApplied LoRA configuration.\ntrainable params: 2,359,296 || all params: 408,650,752 || trainable%: 0.5773\nMoving main model to cuda...\nMain model is now on device: cuda:0\nSummarizer checkpoints will be saved to: /kaggle/working/checkpoints/summarizer\n\n######################### Summarizer Epoch: 1/1 #########################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Summarizer Epoch 1 Training:   0%|          | 0/1118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65df404990f34874bd44b0c5aaff9780"}},"metadata":{}},{"name":"stdout","text":"Memory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nMemory cleaned up.\nSummarizer Epoch 1 Average Train Loss: 3.0905\nSummarizer validation processing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf88f41ea4d4900b6325932db513f12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Summarizer Validation:   0%|          | 0/240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"696560b61bb143d9a579ad7c4afbccf7"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}